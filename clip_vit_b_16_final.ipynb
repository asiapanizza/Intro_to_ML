{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1478015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and check device\n",
    "\n",
    "import os, platform, json, torch, faiss, numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available(): #check if CUDA-enabled GPU is present\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():  #for Mac users, check if MPS is available for high performance computing\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\" #otherwise, CPU\n",
    "print(f\"‚úì running on ‚áí {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts info in case of CUDA-enabled GPU\n",
    "\n",
    "if device == \"cuda\":\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:                                  \n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        print(\"‚úì TF32 enabled\")\n",
    "\n",
    "    print(f\"CUDA toolkit  : {torch.version.cuda}\")          \n",
    "    try:\n",
    "        drv = torch._C._cuda_getDriverVersion()            \n",
    "        print(f\"GPU driver   : {drv//1000}.{(drv//10)%100}\") \n",
    "    except AttributeError:\n",
    "        print(\"GPU driver   : <unavailable>\")\n",
    "    print(f\"GPU name      : {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CLIP backbone with open-clip\n",
    "\n",
    "import open_clip, torch, re\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-16\",\n",
    "        pretrained=\"laion400m_e32\",\n",
    "        device=device)\n",
    "backbone = model.visual      \n",
    "backbone.eval()                  # encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b847037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# right after backbone = model.visual\n",
    "feature_dim = backbone.output_dim        # 512 for B/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspects layers in the model\n",
    "import torch\n",
    "linear_names = [n for n, m in backbone.named_modules() if isinstance(m, torch.nn.Linear)]\n",
    "print(\"Found\", len(linear_names), \"Linear layers:\")\n",
    "for n in linear_names[:40]: #show first 40 layers just to get an idea\n",
    "    print(\"  \", n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the purpose of PEFT is to allow training a small number of parameters\n",
    "# PEFT might pass input_ids, so we need to wrap the backbone to allow passing pixel_values\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class ViTForPeft(nn.Module): #since the class derives from nn.Module, it will work as a neural network module in Pytorch \n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone #pre-trained CLIP backbone\n",
    "    def forward(self, pixel_values=None, input_ids=None, **kwargs):\n",
    "        #will make sure backbone always receives image data through pixel_values\n",
    "        #images will be passed as pixel_values in the (B√ó3√óH√óW) format\n",
    "        if pixel_values is None:\n",
    "            pixel_values = input_ids          # backup plan if pixel_values is not provided\n",
    "        return self.backbone(pixel_values)    # standard OpenCLIP call\n",
    "\n",
    "backbone_peft = ViTForPeft(backbone)          # now we can use the backbone with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will return a list of valid LoRA targets - i.e., layers that can be adapted with LoRA.\n",
    "#LoRA is a technique that allows training a small number of parameters while keeping the rest of the model frozen.\n",
    "def get_valid_lora_targets(model):\n",
    "    valid_types = (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d, torch.nn.MultiheadAttention)\n",
    "    targets = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, valid_types):\n",
    "            targets.append(name)\n",
    "    return targets\n",
    "\n",
    "valid_targets = get_valid_lora_targets(backbone)\n",
    "print(valid_targets) #parameters that can be adapted with LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "final_targets = [] \n",
    "lorizzabili = valid_targets  # the list of valid LoRA targets we previously obtained \n",
    "#we will use this list to determine the final targets for LoRA adaptation\n",
    "\n",
    "# Using the Original Model (without PEFT) to Determine Targets\n",
    "print(\"üîç Determinazione target usando il modello originale...\")\n",
    "for block_idx, block in enumerate(backbone.transformer.resblocks[-4:]): #we want to focus on the last four blocks of the transformer\n",
    "    for name, module in block.named_modules(): \n",
    "        if isinstance(module, torch.nn.Linear): \n",
    "            full_name = f\"transformer.resblocks.{len(backbone.transformer.resblocks)-4+block_idx}.{name}\" \n",
    "            if full_name in lorizzabili: #double check if the full name is in the list of valid LoRA targets\n",
    "                final_targets.append(full_name) \n",
    "\n",
    "final_targets = list(set(final_targets)) #removes duplicates\n",
    "print(f\"üéØ Target LoRA finali: {final_targets}\") #all the final parameters to which we will apply LoRA\n",
    "\n",
    "# Creating the LoRA Configuration\n",
    "peft_cfg_fixed = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION, #main job of our model is feature extraction\n",
    "    r=16, #rank of the LoRA layers, determines the number of parameters to be added to the model, 16 is a small value that allows for efficient adaptation\n",
    "    lora_alpha=16, #scaling factor for the LoRA layers, generally set to the same value as r\n",
    "    lora_dropout=0.05, #turns on dropout for the LoRA layers, helps prevent overfitting\n",
    "    target_modules=final_targets, #list of target modules to which LoRA will be applied\n",
    ")\n",
    "\n",
    "# create wrapper PEFT with the right configuration\n",
    "print(\"üîß Creazione wrapper PEFT con target corretti...\")\n",
    "backbone_peft = get_peft_model(backbone, peft_cfg_fixed)\n",
    "\n",
    "# double check that LoRA is applied correctly within the wrapper \n",
    "print(\"\\n‚úÖ Verifica moduli LoRA applicati:\")\n",
    "lora_modules = []\n",
    "for name, module in backbone_peft.named_modules():\n",
    "    if hasattr(module, 'lora_A') or 'lora' in str(type(module)).lower(): #if a module has a lora_A or lora attribute, it means LoRA was applied to it\n",
    "        lora_modules.append(name)\n",
    "\n",
    "print(f\"\\nüìä Totale moduli LoRA applicati: {len(lora_modules)}\")\n",
    "print(f\"üìä Target richiesti: {len(final_targets)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc87400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the last 4 blocks (blocks[-4:])\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# enables gradient checkpointing for memory efficiency\n",
    "target_blocks = [8, 9, 10, 11]\n",
    "for i in target_blocks:\n",
    "    model.visual.transformer.resblocks[i].gradient_checkpointing = True\n",
    "model.visual.transformer.gradient_checkpointing = True\n",
    "\n",
    "# Freeze all parameters except the last 4 blocks\n",
    "vit_blocks = list(backbone_peft.transformer.resblocks)\n",
    "for blk in vit_blocks[:-4]:\n",
    "    for name, param in blk.named_parameters():\n",
    "        if 'lora_' not in name:\n",
    "            param.requires_grad = False\n",
    "#checks trainable parameters\n",
    "backbone_peft.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# robustly get feature dimension of a backbone\n",
    "def get_feature_dim(b):\n",
    "    if hasattr(b, \"embed_dim\"): #if the backbone has an embed_dim attribute\n",
    "        return b.embed_dim\n",
    "    if hasattr(b, \"output_dim\"): #if the backbone has an output_dim attribute\n",
    "        return b.output_dim\n",
    "    return b.proj.shape[1]        # last resort, if the backbone has a proj attribute, return its second dimension\n",
    "\n",
    "\n",
    "class CLIPProbe(nn.Module): #part of our neural network\n",
    "    def __init__(self, backbone_peft, prototype_dim=512): # size of the output feature vector (it depends on the model)\n",
    "        self.backbone_peft = backbone_peft #    LoRA tuned backbone with PEFT will be stored here\n",
    "        vision_feature_dim = 512 \n",
    "        self.projector = nn.Sequential( #we build a projector to map the features to the prototype dimension\n",
    "            nn.Linear(vision_feature_dim, 512), \n",
    "            nn.ReLU(),         #makes network non-linear, thus able to learn more complext functions                  \n",
    "            nn.Dropout(0.1),                    # Dropout to prevent overfitting\n",
    "            nn.Linear(512, prototype_dim)       \n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone_peft(pixel_values=x) #x, aka the image, is passed through the backbone with LoRA\n",
    "        return nn.functional.normalize(self.projector(x), dim=-1) #then the image is passed through the projector and normalized along the last dimension of the output vector (for better comparability of the features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cdc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from PIL import ImageFile\n",
    "\n",
    "#quick set up of our training and validation datasets\n",
    "# loads images even if they are truncated or corrupted\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#identify directories\n",
    "train_dir   = Path(\"train\")\n",
    "query_dir   = Path(\"test/query\")\n",
    "gallery_dir = Path(\"test/gallery\")\n",
    "\n",
    "\n",
    "full_set = ImageFolder(train_dir, transform=preprocess) # training images are preprocessed aka cropped, resized, normalized and converted to a tensor\n",
    "#extracts all labels from the dataset\n",
    "labels_all = full_set.targets  \n",
    "\n",
    "print(f\"Dataset size: {len(full_set)} images\")\n",
    "print(f\"Number of classes: {len(set(labels_all))}\")\n",
    "\n",
    "#dataset splitting\n",
    "# get all indices\n",
    "idx_all = list(range(len(full_set)))\n",
    "\n",
    "# splits the indices\n",
    "train_idx, val_idx = train_test_split(\n",
    "    idx_all, \n",
    "    test_size=0.20,\n",
    "    stratify=labels_all, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_set = Subset(full_set, train_idx)\n",
    "val_set = Subset(full_set, val_idx)\n",
    "\n",
    "print(f\"Train set: {len(train_set)} images\")\n",
    "print(f\"Val set: {len(val_set)} images\")\n",
    "\n",
    "# sets to use CPU workers to load and preprocess images while GPU is training\n",
    "num_workers = min(os.cpu_count(), 8)  # Cap at 8 to avoid diminishing returns\n",
    "print(f\"‚öôÔ∏è Using {num_workers} workers\")\n",
    "\n",
    "# Optimize batch size based on available memory\n",
    "def get_optimal_batch_size():\n",
    "    \"\"\"Determine optimal batch size based on available GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Get GPU memory\n",
    "        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        if gpu_memory_gb >= 16:\n",
    "            return 256  # High-end GPU\n",
    "        elif gpu_memory_gb >= 8:\n",
    "            return 128  # Mid-range GPU\n",
    "        else:\n",
    "            return 64   # Lower-end GPU\n",
    "    else:\n",
    "        return 32  # CPU fallback\n",
    "\n",
    "optimal_batch_size = get_optimal_batch_size()\n",
    "print(f\"üéØ Using batch size: {optimal_batch_size}\")\n",
    "\n",
    "#set up dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=optimal_batch_size,\n",
    "    shuffle=True, #images are shuffled for better training\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),  # Pin memory for faster GPU transfer\n",
    "    persistent_workers=True,               # Keep workers alive between epochs\n",
    "    prefetch_factor=2,                     # worker must load and preprocess 2 batches in advance\n",
    "    drop_last=True                         # Drop incomplete batch for consistent training (trains on full batches only, if some images are left they are discarded)\n",
    ")\n",
    "\n",
    "#same as above\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=optimal_batch_size,\n",
    "    shuffle=False, #images are not shuffled for validation\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    drop_last=True \n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcbbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss, optimiser\n",
    "dim      = feature_dim                  # use the cached value\n",
    "model_ft = CLIPProbe(backbone_peft, dim).to(device)\n",
    "\n",
    "# counts number of classes in the dataset\n",
    "num_classes = len(full_set.classes)          \n",
    "cls_prot    = nn.Parameter(torch.empty(num_classes, dim, device=device)) #defines a learnable parameter for the class prototypes\n",
    "nn.init.xavier_uniform_(cls_prot)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss() #we use CrossEntropyLoss\n",
    "\n",
    "#checks trainable parameters (I had some issues with this, this is some additional debugging to be on the safe side)\n",
    "for n, p in model_ft.backbone_peft.named_parameters():\n",
    "    if \"lora_\" in n:\n",
    "        p.requires_grad_(True)\n",
    "        print(f\"LoRA unfrozen for training: {n}\") #to check\n",
    "    else: \n",
    "        p.requires_grad_(False)\n",
    "#trainable LoRA parameters\n",
    "lora_params_final = [p for n, p in model_ft.named_parameters() if 'lora_' in n and p.requires_grad] # Sar√† vuoto\n",
    "#trainable projector parameters\n",
    "projector_params_final = [p for p in model_ft.projector.parameters() if p.requires_grad]\n",
    "\n",
    "print(f\"DEBUG: Parametri LoRA allenabili: {len(lora_params_final)}\") \n",
    "print(f\"DEBUG: Parametri Projector allenabili: {len(projector_params_final)}\")\n",
    "\n",
    "#sets up different learning rates for LoRA and projector\n",
    "# LoRA parameters are trained with a lower learning rate than the projector\n",
    "optim = torch.optim.AdamW([\n",
    "    {'params': lora_params_final, 'lr': 1e-4}, \n",
    "    {'params': projector_params_final, 'lr': 5e-4}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820df1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we initialize a scheduler for the optimizer, which will adjust the learning rate during training\n",
    "#the lr will be adjusted from the intial value to almost zero during training\n",
    "num_epochs_total = 5 # must correspond to the number of epochs for the training loop\n",
    "num_training_steps_total = len(train_loader) * num_epochs_total\n",
    "\n",
    "warmup_steps = int(0.1 * num_training_steps_total) \n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optim,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps_total,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa99530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "num_epochs = num_epochs_total   # big upper bound ‚Äì early-stop will cut it short\n",
    "\n",
    "patience    = 3 #epochs without improvement before early stopping\n",
    "delta       = 1e-4    # min improvement to reset patience\n",
    "\n",
    "train_loss_hist, val_loss_hist = [], []\n",
    "val_acc_hist    = []\n",
    "\n",
    "best_val = float('inf')\n",
    "epochs_no_imp = 0\n",
    "\n",
    "# adaptive accumulation steps\n",
    "accumulation_steps = 1\n",
    "max_accumulation_steps = 8\n",
    "threshold_increase = 0.05\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    # TRAINING\n",
    "    model_ft.train() #model wiyh LoRA is set to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device) #images and labels from CPU to GPU\n",
    "        vision_output = model_ft.backbone_peft.base_model.model(imgs) #images fed to the backbone\n",
    "        feats = vision_output.detach().clone().requires_grad_(True) #features are extracted from the backbone, detached from the graph and set to require gradients\n",
    "        feats = model_ft.projector(feats) #features are extracted and passed through the projector that transforms them to the prototype dimension\n",
    "        feats = torch.nn.functional.normalize(feats, dim=-1) #features are normalized along the last dimension for better comparability\n",
    "        logits = feats @ cls_prot.T #matrix multiplication between the features and the class prototypes to get the logits (raw predictions)\n",
    "        loss   = criterion(logits, labels) #calculates loss for the current batch using logits and labels\n",
    "        loss = loss / accumulation_steps # divide loss by accumulation steps to average it over multiple batches\n",
    "        loss.backward() #backpropagation to compute gradients\n",
    "        if (i + 1) % accumulation_steps == 0: #if we reached the accumulation steps\n",
    "            optim.step() #update the model parameters\n",
    "            optim.zero_grad() #reset gradients to zero\n",
    "            scheduler.step() # update the learning rate according to the scheduler (now it's linear)\n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "    avg_train = running_loss / len(train_loader)\n",
    "    train_loss_hist.append(avg_train)\n",
    "\n",
    "    #basically same as above, but with validation data\n",
    "    # VALIDATION\n",
    "    model_ft.eval()\n",
    "    v_loss, v_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            vision_output = model_ft.backbone_peft.base_model.model(imgs)\n",
    "            feats = vision_output.detach().clone().requires_grad_(True) \n",
    "            feats = model_ft.projector(feats)\n",
    "            feats = torch.nn.functional.normalize(feats, dim=-1)\n",
    "\n",
    "            logits = feats @ cls_prot.T\n",
    "            v_loss += criterion(logits, labels).item()\n",
    "            v_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_val = v_loss / len(val_loader)\n",
    "    val_loss_hist.append(avg_val)\n",
    "    val_acc  = v_correct / len(val_set)\n",
    "    val_acc_hist.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} ‚îÇ \"\n",
    "              f\"train loss {avg_train:.4f} ‚îÇ \"\n",
    "              f\"val loss {avg_val:.4f} ‚îÇ \"\n",
    "              f\"val acc {val_acc:.3%}\")\n",
    "    #EARLY STOP\n",
    "    if avg_val < best_val - delta:\n",
    "        best_val = avg_val\n",
    "        epochs_no_imp = 0\n",
    "        torch.save(\n",
    "            {\"model_ft\": model_ft.state_dict(),\n",
    "             \"cls_prot\": cls_prot.detach().cpu()},\n",
    "            \"best_probe.pt\")\n",
    "        print(\"üíæ Saved best model checkpoint\")\n",
    "    else:\n",
    "        epochs_no_imp += 1\n",
    "        if epochs_no_imp >= patience:\n",
    "            print(f\"üõë Early stop at epoch {epoch} ‚Äì no val-improvement for {patience} rounds.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# PLOT train & val curves    \n",
    "plt.figure()\n",
    "plt.plot(train_loss_hist, label=\"train loss\", marker='o')\n",
    "plt.plot(val_loss_hist,   label=\"val loss\",   marker='s')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "best_epoch = 1 + val_loss_hist.index(min(val_loss_hist))\n",
    "print(f\"‚úî Best epoch (lowest val loss): {best_epoch}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_acc_hist, marker='^')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd155304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best weights before inference\n",
    "\n",
    "# in this way we use the weights of the best epoch number for inference\n",
    "\n",
    "ckpt = torch.load(\"best_probe.pt\", map_location=device)\n",
    "model_ft.load_state_dict(ckpt[\"model_ft\"])\n",
    "cls_prot.data.copy_(ckpt[\"cls_prot\"].to(device))\n",
    "model_ft.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Ensure 'model', 'preprocess', and 'device' are available from your previous cells.\n",
    "\n",
    "def embed(image_path: Path, model, preprocess_fn, device: str):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Preprocess the image and add a batch dimension\n",
    "    image_input = preprocess_fn(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        # This will internally call model.visual.forward(image_input), aka PEFT-wrapped ViTForPeft instance.\n",
    "        image_features = model.encode_image(image_input)\n",
    "    # Normalize features and convert to NumPy array\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "def embed_folder(folder_path: Path, model, preprocess_fn, device: str, pattern: str = \"*.jpg\"):\n",
    "    vecs, names = [], []\n",
    "    # Use tqdm for a progress bar as embedding a folder can take time\n",
    "    for p in tqdm(sorted(folder_path.glob(pattern)), desc=f\"Embedding {folder_path.name}\"):\n",
    "        vecs.append(embed(p, model, preprocess_fn, device))\n",
    "        names.append(p.name)\n",
    "    return np.vstack(vecs), names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb26724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings generation\n",
    "print(\"Generating gallery embeddings...\")\n",
    "gallery_vecs, gallery_files = embed_folder(gallery_dir, model, preprocess, device)\n",
    "print(f\"Generated {len(gallery_vecs)} gallery embeddings. Shape: {gallery_vecs.shape}\")\n",
    "\n",
    "print(\"\\nGenerating query embeddings...\")\n",
    "query_vecs, query_files = embed_folder(query_dir, model, preprocess, device)\n",
    "print(f\"Generated {len(query_vecs)} query embeddings. Shape: {query_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d143e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faiss performs L2 normalization on the vectors to ensure they are unit vectors\n",
    "faiss.normalize_L2(gallery_vecs)\n",
    "faiss.normalize_L2(query_vecs)\n",
    "\n",
    "index = faiss.IndexFlatIP(gallery_vecs.shape[1]) #finds vectors with the highest inner product (IP) similarity (cosine similarity)\n",
    "index.add(gallery_vecs)\n",
    "\n",
    "k = 10   #returns the 10 most simmilar images\n",
    "_, neigh = index.search(query_vecs, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#JSON SUBMISSION\n",
    "entries = []\n",
    "for qi, row in enumerate(neigh):\n",
    "    hits = [gallery_files[j] for j in row]          \n",
    "    entries.append({\"filename\": query_files[qi],\n",
    "                    \"samples\":  hits})\n",
    "\n",
    "with open(\"submission.json\", \"w\") as f:\n",
    "    json.dump(entries, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ submission.json saved  |  queries:\", len(entries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a1167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visual sanity-check\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def show_query(idx):\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    # show query\n",
    "    q_img = Image.open(query_dir / query_files[idx])\n",
    "    plt.subplot(1, k+1, 1); plt.imshow(q_img); plt.axis(\"off\"); plt.title(\"query\")\n",
    "    # show retrieved gallery images\n",
    "    for i, j in enumerate(neigh[idx], start=2):\n",
    "        g_img = Image.open(gallery_dir / gallery_files[j])\n",
    "        plt.subplot(1, k+1, i); plt.imshow(g_img); plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "for _ in range(15):\n",
    "    show_query(random.randrange(len(query_files)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
